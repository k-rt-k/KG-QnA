{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHm0npUwx4xZ",
        "outputId": "d9fc2dce-d3b4-4f1d-cfa7-dd4dc815216e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.3.5)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.10/dist-packages (2.2.1)\n"
          ]
        }
      ],
      "source": [
        "# Run in colab\n",
        "!pip install torch\n",
        "!pip install openai\n",
        "!pip install transformers\n",
        "!pip install backoff\n",
        "\n",
        "attr_to_num = {'movie2actor':0,'movie2genre':1,'movie2director':2,'movie2year':3,'movie2desc':4}\n",
        "\n",
        "import json\n",
        "## code for loading and training the language model\n",
        "import transformers\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import openai\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import backoff\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def node_to_ans(nodename:str)->str:\n",
        "    return nodename.split(':')[1].replace('_',' ')\n",
        "\n",
        "def dataset_to_csv(input_file:str='dataset.txt',output_file:str='dataset.csv')->None:\n",
        "    with open(input_file,'r') as f:\n",
        "        lines = f.readlines()\n",
        "        qlines = lines[::3]\n",
        "        answers = lines[1::3]\n",
        "    qlines = [q.strip() for q in qlines if q.strip()!='']\n",
        "    answers = [a.split(':')[1].strip() for a in answers if a.strip()!='']\n",
        "    df = pd.DataFrame({'queries':qlines,'answers':answers})\n",
        "    df.to_csv(output_file)\n",
        "    print('done')\n",
        "    return\n",
        "\n",
        "sysp='''\n",
        "The task for you is to generate a list of tuples which denotes various relations from the query. The relation types are only: \"movie2actor\", \"movie2director\", \"movie2year\", \"movie2genre\". For entities having known value, use the value, and for unknown entities, use placeholders starting with a ?. You need to decide the unknown entities yourselves. Here are a few examples:\n",
        "\n",
        "Query: Which movie was released in 2015 starring Leonardo Di Caprio?\n",
        "\n",
        "Answer: [(?movie1, movie2actor, \"Leonardo Di Caprio\")]\n",
        "\n",
        "Query: Which movie released in 2010 shares an actor which the movie Inception?\n",
        "\n",
        "Answer: [(?movie1, movie2year, \"2010\"), (?movie1, movie2actor, ?actor1), (\"Inception\", movie2actor, ?actor1)]\n",
        "\n",
        "In a query I might ask multiple subqueries, with each one separated by '||', return the output for each of the subqueries, again separated by '||'\n",
        "The answer should strictly match the format in which I specified.  Do not add any other relations than what I mentioned. For example, do not add a relation of the type movie2title in the tuple. Also, try to be as minimal with the list as possible.\n",
        "'''\n",
        "@backoff.on_exception(backoff.expo, openai.RateLimitError)\n",
        "def ask(query,client):\n",
        "  query = \"||\".join(query)\n",
        "  rp = client.chat.completions.create(\n",
        "      model=\"gpt-3.5-turbo-1106\",\n",
        "      messages=[\n",
        "          {\"role\":\"system\", \"content\":sysp},\n",
        "          {\"role\": \"user\", \"content\": query},\n",
        "      ]\n",
        "  )\n",
        "  return rp\n",
        "\n",
        "rps= []\n",
        "import time\n",
        "### somehow scrape dataset.txt for finetuning the lm ###\n",
        "def get_lm_train_data(dataset_file:str='dataset.csv',add_desc:bool=False,save_to_csv:bool=True)->pd.DataFrame:\n",
        "    openaiapi_key = os.environ.get('OPENAI_KEY','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')\n",
        "    # openaiapi_key = None\n",
        "    # if openaiapi_key is None:\n",
        "    #     raise Exception(\"Set env OPENAI_KEY to your OpenAI Key\")\n",
        "\n",
        "    client = openai.OpenAI(\n",
        "      api_key=openaiapi_key,  # this is also the default, it can be omitted\n",
        "    )\n",
        "\n",
        "    df = pd.read_csv(dataset_file)\n",
        "    qlines = df['queries'].to_list()\n",
        "\n",
        "    if add_desc:\n",
        "        #desc_prompt =\n",
        "        responses = []\n",
        "        descriptions = []\n",
        "\n",
        "        # df['sparqls']= responses\n",
        "        # df['descs']= descriptions\n",
        "        raise NotImplementedError\n",
        "    else: ## no desc\n",
        "        # prmpt = lambda query:f'''Given queries enclosed in arrows, convert them into the SPARQL language in order to query over a knowledge graph containing nodes for 'actor','director','movie', 'genre', 'year'. Each node name is prefixed by its type, and contains underscores instead of spaces. For example actor Michael Scott's node reads 'actor:Michael_Scott'. Each relation is one out of {' '.join(attr_to_num.keys())}, with the edge pointing in the appropriate direction.\n",
        "        # You may think over your answer, but your final answer for each query must be enclosed in triple slashes '/// ///'.\n",
        "\n",
        "        # The queries are :\n",
        "        # {query}\n",
        "\n",
        "        # The responses are:\n",
        "        # '''\n",
        "        responses = []\n",
        "\n",
        "        # give k queries at a time\n",
        "        k = 1\n",
        "        # qb = ['\\n'.join([f'<<<{q}>>>'for q in qlines[i:i+k]]) for i in range(0, len(qlines), k)]\n",
        "        qb = qlines\n",
        "        bunch = []\n",
        "        for i,query in tqdm(enumerate(qb)):\n",
        "            bunch.append(query)\n",
        "            if (i+1)%10 == 0:\n",
        "              rp = ask(bunch,client)\n",
        "              bunch = []\n",
        "              time.sleep(20)\n",
        "              for j in rp.choices[0].message.content.split(\"||\\n\"):\n",
        "                rps.append(j)\n",
        "            # print(cntnt)\n",
        "            # responses += ans\n",
        "            #print(i)\n",
        "\n",
        "        # df['sparqls']=responses\n",
        "\n",
        "    if save_to_csv:\n",
        "        df.to_csv(dataset_file)\n",
        "    return df"
      ],
      "metadata": {
        "id": "TeC5Qvp7yQNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_to_csv('dataset.txt','dataset0.csv')\n"
      ],
      "metadata": {
        "id": "g1zGYmcMyQ6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_lm_train_data('dataset0.csv')"
      ],
      "metadata": {
        "id": "InmK5S89yaIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%debug"
      ],
      "metadata": {
        "id": "WezEukqeEVDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(rps))"
      ],
      "metadata": {
        "id": "y14FkLL8v5zE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}